{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"1\">Machine Learning Algorithms for Digital Wage Payment Prediction</h1> \n",
    "<h2 style=\"text-align: center;\" markdown=\"2\">An AIMS Masters project in Collaboration with the Global Centre on Digital Wages for Decent Work (ILO)</h2>\n",
    "\n",
    "\n",
    "> *The widespread adoption of digital payments has become increasingly important. In light of this, the aim of this project is to investigate the probability of an individual receiving digital wages in Africa. To achieve this, a series of empirical comparative assessments of machine learning classification algorithms will be conducted. The objective is to determine the effectiveness of these algorithms in predicting digital wage payments. Therefore, this notebook forms part of a larger project that seeks to explore the potential of machine learning in addressing issues related to financial inclusion in Africa.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\" markdown=\"3\"> Final Data Preparation</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Table of Contents\n",
    "[Load Raw Data](#load-data)  \n",
    "[Inspect and Clean Data](#inspect)      \n",
    "[Create Test/Train Split and Save Data](#save-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the Raw Data <a class=\"anchor\" id=\"load-data\"></a>\n",
    "\n",
    "First, we load a few essential modules used in notebook. We have developed several utility functions in the `load_data.py` file located in the `src/data` directory that will be used throughout this project for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Add local functions to the path\n",
    "sys.path.append(os.path.join(os.pardir, 'src'))\n",
    "from data import load_data\n",
    "from features import process_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and Clean Data <a class=\"anchor\" id=\"inspect\"></a>\n",
    "\n",
    "To begin, we will load the data. The data is in a uniform format, allowing us to develop useful functions that can be applied to all datasets.\n",
    "\n",
    "For this project, we are utilizing survey data that has been mostly cleaned and formatted into CSV files. However, there are a few outstanding issues that we will address as we create the data loading function.\n",
    "\n",
    "\n",
    "1. To ensure that the categorical variables are not read as numeric, we convert all variables to their appropriate data types after reading them using `pandas.read_csv()`.\n",
    "\n",
    "1. Our dataset's target variable is `receive_wages`, and since this is a binary classification task, we only need two categories. However, this variable has three categories. To simplify this, we can merge categories 1 (`received payments into an account`) and 3 (`received payments using other methods`) into a single category, which will represent those who received their wages digitally. This category is coded as 1 thereafter.\n",
    "\n",
    "1. The `age` variable contains single-year values, which we can group into more meaningful categories based on the [UN's recommended standard international age classifications](https://unstats.un.org/unsd/publication/seriesm/seriesm_74e.pdf). Since the minimum age in the dataset is 15, we group the ages into four categories: 15-24 (youth), 25-44 (young adulthood), 45-64 (middle adulthood), and 65+ (older adulthood).\n",
    "\n",
    "1. The first column in the dataset is unnamed and represents the row position of each observation in the original uncleaned dataset (`data/raw/GLOBAL/global_data_2021.csv`). We rename it as `id` and set it as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_file(filepath):\n",
    "    \n",
    "    \"\"\" \n",
    "    Load data in correct format from CSV file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : a filepath to the file to be loaded\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data: the processed dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filepath)\n",
    "    \n",
    "    #drop the regionwb column\n",
    "    data.drop(['regionwb', 'receive_welfare_payments'], axis=1, inplace = True)\n",
    "    \n",
    "    #define function to categorize age\n",
    "    def categorize_age(data):\n",
    "        data.loc[(data['age'] >= 15) & (data['age'] <= 24), 'age'] = 1\n",
    "        data.loc[(data['age'] >= 25) & (data['age'] <= 44), 'age'] = 2\n",
    "        data.loc[(data['age'] >= 45) & (data['age'] <= 64), 'age'] = 3\n",
    "        data.loc[data['age'] >= 65, 'age'] = 4\n",
    "        data['age'] = data['age'].astype('category')\n",
    "        return data\n",
    "    # call the function and assign the returned value back to data\n",
    "    data = categorize_age(data)\n",
    "    \n",
    "    # convert those who responded to receiving wages through other methods to digital wage receipients\n",
    "    # and rename the variable from receive_wages to receive_digital_wages\n",
    "    data.loc[data.receive_wages == 3, 'receive_wages'] = 1\n",
    "    data.rename( columns={'receive_wages':'receive_digital_wages'}, inplace=True )\n",
    "    \n",
    "    # rename the first column to be id (referencing row position in original uncleaned data) and setting it as index\n",
    "    data.rename( columns={'Unnamed: 0':'id'}, inplace=True )\n",
    "    data.set_index('id', inplace = True)\n",
    "    \n",
    "    numeric_variables = ['pop_scaled_wgt']\n",
    "    # convert the categorical variables into the category type\n",
    "    for c in data.columns:\n",
    "        if c not in numeric_variables:\n",
    "            data[c] = data[c].astype('category')\n",
    "    \n",
    "    #Drop Algeria, Gabon, Mauritius and Morocco due to missing values not at random\n",
    "    data.dropna(subset = ['urbanicity_f2f'], inplace=True)\n",
    "    \n",
    "    data['economy'] = data['economy'].cat.remove_unused_categories()\n",
    "      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def knn_imputation(df, k=3, impute_type='F'):\n",
    "#     '''' \n",
    "#     Performs K-Nearest Neighbors imputation on a pandas DataFrame with missing categorical values. \n",
    "#     The function takes in a pandas DataFrame with missing values in categorical columns. \n",
    "#     The function uses KNNImputer from scikit-learn to impute the missing values by computing \n",
    "#     distances between the samples in the dataset and their nearest neighbors. \n",
    "    \n",
    "#     Parameters: \n",
    "#     --------------\n",
    "#     df: pandas DataFrame with missing categorical values. \n",
    "#     k: number of nearest neighbors to use when performing imputation (default=3).\n",
    "#     impute_age: boolean value indicating whether to impute missing values in the 'age' column (default=False).\n",
    "#                 - if F, the function will impute missing values in the 'age' column along with the other \n",
    "#                     categorical columns. \n",
    "#                 - if D, the function will only impute missing values in the 'age' column\n",
    "#                 - if the value is neither F nor D, an error message will be returned. \n",
    "                \n",
    "#     Returns:\n",
    "#     -------------\n",
    "#     df: a pandas DataFrame with missing categorical values imputed using KNNImputer\n",
    "    \n",
    "#     '''\n",
    "    \n",
    "#     if impute_type not in ['F', 'D']:\n",
    "#         raise ValueError(\"Invalid value for impute_type. Expected 'F' or 'D', but got {}\".format(impute_type))\n",
    "\n",
    "#     if impute_type == 'F':\n",
    "#         categorical_cols = ['fin45', 'age', 'economy']\n",
    "#     else:\n",
    "#         categorical_cols = ['age', 'economy']\n",
    "\n",
    "#     data = pd.get_dummies(df, columns=categorical_cols)\n",
    "\n",
    "#     imputer = KNNImputer(n_neighbors=k)\n",
    "#     imputed_data = imputer.fit_transform(data)\n",
    "#     imputed_data = pd.DataFrame(imputed_data, columns=data.columns)\n",
    "\n",
    "#     if impute_type == 'F':\n",
    "#         imputed_data['fin45'] = np.argmax(imputed_data[['fin45_1.0', 'fin45_2.0', 'fin45_3.0', \n",
    "#                                                         'fin45_4.0', 'fin45_5.0']].values, axis=1)\n",
    "\n",
    "#     imputed_data['age'] = np.argmax(imputed_data[['age_1.0', 'age_2.0', 'age_3.0', 'age_4.0']].values, axis=1)\n",
    "\n",
    "#     if impute_type == 'F':\n",
    "#         df = imputed_data.drop(['fin45_1.0', 'fin45_2.0', 'fin45_3.0','fin45_4.0', 'fin45_5.0'], axis=1)\n",
    "#     else:\n",
    "#         df = imputed_data.copy()\n",
    "\n",
    "#     df = df.drop(['age_1.0', 'age_2.0', 'age_3.0', 'age_4.0'], axis=1)\n",
    "    \n",
    "#     numeric_variables = ['pop_scaled_wgt']\n",
    "    \n",
    "#     # convert the categorical variables into the category type\n",
    "#     for c in df.columns:\n",
    "#         if c not in numeric_variables:\n",
    "#             df[c] = df[c].astype('category')\n",
    "            \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the datasets using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5806 entries, 9023 to 127848\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   economy                5806 non-null   category\n",
      " 1   pop_scaled_wgt         5806 non-null   float64 \n",
      " 2   female                 5806 non-null   category\n",
      " 3   age                    5797 non-null   category\n",
      " 4   educ                   5806 non-null   category\n",
      " 5   inc_q                  5806 non-null   category\n",
      " 6   emp_in                 5806 non-null   category\n",
      " 7   urbanicity_f2f         5806 non-null   category\n",
      " 8   account                5806 non-null   category\n",
      " 9   fin14_1                5806 non-null   category\n",
      " 10  fin16                  5806 non-null   category\n",
      " 11  fin17a                 5806 non-null   category\n",
      " 12  fin17b                 5806 non-null   category\n",
      " 13  fin22a                 5806 non-null   category\n",
      " 14  fin22b                 5806 non-null   category\n",
      " 15  fin24                  5806 non-null   category\n",
      " 16  fin33                  5806 non-null   category\n",
      " 17  receive_digital_wages  5806 non-null   category\n",
      " 18  pay_utilities          5806 non-null   category\n",
      " 19  remittances            5806 non-null   category\n",
      " 20  mobileowner            5806 non-null   category\n",
      " 21  internetaccess         5806 non-null   category\n",
      " 22  merchantpay_dig        5806 non-null   category\n",
      " 23  internet_fin_transc    5806 non-null   category\n",
      "dtypes: category(23), float64(1)\n",
      "memory usage: 225.1 KB\n",
      "The full feature dataset has 5,806 rows and 24 columns\n",
      "Percentage Receiving Digital Wages: 63.5% \tPercentage Not Receiving Digital Wages: 36.5%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economy</th>\n",
       "      <th>pop_scaled_wgt</th>\n",
       "      <th>female</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>inc_q</th>\n",
       "      <th>emp_in</th>\n",
       "      <th>urbanicity_f2f</th>\n",
       "      <th>account</th>\n",
       "      <th>fin14_1</th>\n",
       "      <th>...</th>\n",
       "      <th>fin22b</th>\n",
       "      <th>fin24</th>\n",
       "      <th>fin33</th>\n",
       "      <th>receive_digital_wages</th>\n",
       "      <th>pay_utilities</th>\n",
       "      <th>remittances</th>\n",
       "      <th>mobileowner</th>\n",
       "      <th>internetaccess</th>\n",
       "      <th>merchantpay_dig</th>\n",
       "      <th>internet_fin_transc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9023</th>\n",
       "      <td>0</td>\n",
       "      <td>4731.949144</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9026</th>\n",
       "      <td>0</td>\n",
       "      <td>4469.830001</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9027</th>\n",
       "      <td>0</td>\n",
       "      <td>17001.509861</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9028</th>\n",
       "      <td>0</td>\n",
       "      <td>9407.418933</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9034</th>\n",
       "      <td>0</td>\n",
       "      <td>4830.758167</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     economy  pop_scaled_wgt female  age educ inc_q emp_in urbanicity_f2f  \\\n",
       "id                                                                          \n",
       "9023       0     4731.949144      2  1.0    2     5    1.0            1.0   \n",
       "9026       0     4469.830001      1  1.0    2     3    1.0            2.0   \n",
       "9027       0    17001.509861      1  3.0    2     5    1.0            2.0   \n",
       "9028       0     9407.418933      2  1.0    2     2    1.0            1.0   \n",
       "9034       0     4830.758167      2  3.0    2     5    1.0            1.0   \n",
       "\n",
       "     account fin14_1  ... fin22b fin24 fin33 receive_digital_wages  \\\n",
       "id                    ...                                            \n",
       "9023       1       2  ...      2     2   2.0                     2   \n",
       "9026       1       2  ...      2     1   2.0                     1   \n",
       "9027       1       2  ...      1     2   1.0                     1   \n",
       "9028       1       2  ...      2     3   2.0                     2   \n",
       "9034       1       2  ...      1     2   1.0                     1   \n",
       "\n",
       "     pay_utilities remittances mobileowner internetaccess merchantpay_dig  \\\n",
       "id                                                                          \n",
       "9023             4         1.0           1              1             0.0   \n",
       "9026             2         1.0           1              2             0.0   \n",
       "9027             1         1.0           1              1             0.0   \n",
       "9028             4         1.0           1              1             0.0   \n",
       "9034             4         1.0           1              2             0.0   \n",
       "\n",
       "     internet_fin_transc  \n",
       "id                        \n",
       "9023                   1  \n",
       "9026                   2  \n",
       "9027                   1  \n",
       "9028                   1  \n",
       "9034                   1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load full feature data\n",
    "filepath = load_data.FULL_MERGED\n",
    "full_merged = load_csv_file(filepath)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_cols =['economy']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    full_merged[col] = encoder.fit_transform(full_merged[col])\n",
    "    \n",
    "full_merged.economy.unique()\n",
    "full_merged[\"economy\"] = full_merged[\"economy\"].astype(\"category\")\n",
    "full_merged.info()\n",
    "\n",
    "\n",
    "# full_merged = knn_imputation(full_merged, k=3, impute_type='D')\n",
    "\n",
    "s = 'The full feature dataset has {:,} rows and {:,} columns'\n",
    "print(s.format(full_merged.shape[0], full_merged.shape[1]))\n",
    "s = 'Percentage Receiving Digital Wages: {:0.1%} \\tPercentage Not Receiving Digital Wages: {:0.1%}'\n",
    "print(s.format(full_merged.receive_digital_wages.value_counts(normalize=True).loc[1], \n",
    "               full_merged.receive_digital_wages.value_counts(normalize=True).loc[2]))\n",
    "full_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.285243\n",
       "4    0.225365\n",
       "3    0.206978\n",
       "2    0.158416\n",
       "1    0.123998\n",
       "Name: inc_q, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = full_merged[full_merged['receive_digital_wages']==2]\n",
    "g.inc_q.value_counts()/g.inc_q.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The demographic-related feature set data has 5,806 rows and 13 columns\n",
      "Percentage Receiving Digital Wages: 63.5% \tPercentage Not Receiving Digital Wages: 36.5%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economy</th>\n",
       "      <th>pop_scaled_wgt</th>\n",
       "      <th>female</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>inc_q</th>\n",
       "      <th>emp_in</th>\n",
       "      <th>urbanicity_f2f</th>\n",
       "      <th>account</th>\n",
       "      <th>fin33</th>\n",
       "      <th>receive_digital_wages</th>\n",
       "      <th>mobileowner</th>\n",
       "      <th>internetaccess</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9023</th>\n",
       "      <td>Benin</td>\n",
       "      <td>4731.949144</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9026</th>\n",
       "      <td>Benin</td>\n",
       "      <td>4469.830001</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9027</th>\n",
       "      <td>Benin</td>\n",
       "      <td>17001.509861</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9028</th>\n",
       "      <td>Benin</td>\n",
       "      <td>9407.418933</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9034</th>\n",
       "      <td>Benin</td>\n",
       "      <td>4830.758167</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     economy  pop_scaled_wgt female  age educ inc_q emp_in urbanicity_f2f  \\\n",
       "id                                                                          \n",
       "9023   Benin     4731.949144      2  1.0    2     5    1.0            1.0   \n",
       "9026   Benin     4469.830001      1  1.0    2     3    1.0            2.0   \n",
       "9027   Benin    17001.509861      1  3.0    2     5    1.0            2.0   \n",
       "9028   Benin     9407.418933      2  1.0    2     2    1.0            1.0   \n",
       "9034   Benin     4830.758167      2  3.0    2     5    1.0            1.0   \n",
       "\n",
       "     account fin33 receive_digital_wages mobileowner internetaccess  \n",
       "id                                                                   \n",
       "9023       1   2.0                     2           1              1  \n",
       "9026       1   2.0                     1           1              2  \n",
       "9027       1   1.0                     1           1              1  \n",
       "9028       1   2.0                     2           1              1  \n",
       "9034       1   1.0                     1           1              2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data with demographic-related features only\n",
    "filepath = load_data.DEM_MERGED\n",
    "dem_merged = load_csv_file(filepath)\n",
    "\n",
    "# dem_merged = knn_imputation(dem_merged, k=3, impute_type='D')\n",
    "\n",
    "s = 'The demographic-related feature set data has {:,} rows and {:,} columns'\n",
    "print(s.format(dem_merged.shape[0], dem_merged.shape[1]))\n",
    "s = 'Percentage Receiving Digital Wages: {:0.1%} \\tPercentage Not Receiving Digital Wages: {:0.1%}'\n",
    "print(s.format(dem_merged.receive_digital_wages.value_counts(normalize=True).loc[1], \n",
    "               dem_merged.receive_digital_wages.value_counts(normalize=True).loc[2]))\n",
    "dem_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert target variable, `receive_digital_wages`, to a Boolean\n",
    "\n",
    "The task at hand is a binary classification problem, hence we need to have a single column named `receive_digital_wages`. This column should be assigned a value of `True` if the individual receives their wages digitally, and `False` if they don't. This can be achieved by changing the data type of the existing dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merged.receive_digital_wages = (full_merged.receive_digital_wages == 1)\n",
    "dem_merged.receive_digital_wages = (dem_merged.receive_digital_wages == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Test Split and Save Data <a class=\"anchor\" id=\"save-data\"></a>\n",
    "\n",
    "As a final step, we want to split the data into training and test sets which will be used by all of the algorithms. We'll reserve 25% of the data as a test set. \n",
    "\n",
    "Each dataset to be analysed will have two associated csv files: train, test.\n",
    "\n",
    "We will create two sets of training and test sets for both the full feature set and demographic-related featureset data. In one set, we will have the categorical variables as they are. In the other, we will create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into Train and Test Sets\n",
    "full_merged_train, full_merged_test = train_test_split(full_merged, \n",
    "                                       test_size=0.25,\n",
    "                                       random_state=1500,\n",
    "                                       stratify=full_merged.receive_digital_wages)\n",
    "\n",
    "dem_merged_train, dem_merged_test = train_test_split(dem_merged, \n",
    "                                       test_size=0.25,\n",
    "                                       random_state=1500,\n",
    "                                       stratify=dem_merged.receive_digital_wages)\n",
    "\n",
    "# Save data to files\n",
    "TRAIN_PATH, TEST_PATH = load_data.get_data_filepaths('full_merged')\n",
    "full_merged_train.to_pickle(TRAIN_PATH)\n",
    "full_merged_test.to_pickle(TEST_PATH)\n",
    "\n",
    "TRAIN_PATH, TEST_PATH = load_data.get_data_filepaths('dem_merged')\n",
    "dem_merged_train.to_pickle(TRAIN_PATH)\n",
    "dem_merged_test.to_pickle(TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy variables <a class=\"anchor\" id=\"dummy-variables\"></a>\n",
    "\n",
    "In the context of classification algorithms, categorical variables may need to be converted to numeric inputs. One popular method for this is creating dummy variables, where a binary column is created for each unique category in the categorical feature. However, it is not necessary to create a dummy variable for every column, as this can lead to multicollinearity issues known as the dummy variable trap. If we have n columns from n categories, every column is actually a linear combination of the other columns, creating the multicollinearity problem. To avoid this, we can drop the first dummy variable for each categorical variable. Pandas provides a useful function called get_dummies() that simplifies this process.\n",
    "\n",
    "In addition to creating dummy variables, we may also need to remove features that are not useful for classification problems, such as empty or constant columns, as well as duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Dropped due to multicolinearity issues (addressed in next section)\n",
    "# full_merged.drop(['regionwb', 'receive_welfare_payments'], axis=1, inplace = True)\n",
    "# dem_merged.drop(['regionwb', 'receive_welfare_payments'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dataset with dummy variables added has shape (5806, 85)\n",
      "Demographic-related feature set data with dummy variables added has shape (5806, 53)\n"
     ]
    }
   ],
   "source": [
    "# create dummy variables for categoricals\n",
    "full_merged = pd.get_dummies(full_merged, drop_first=True, dummy_na=True, prefix_sep='__')\n",
    "dem_merged = pd.get_dummies(dem_merged, drop_first=True, dummy_na=True, prefix_sep='__')\n",
    "\n",
    "print(\"Full feature dataset with dummy variables added has shape\", full_merged.shape)\n",
    "print(\"Demographic-related feature set data with dummy variables added has shape\", dem_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dataset with constant columns dropped has shape (5806, 64)\n",
      "Demographic-related feature set data with constant columns dropped has shape (5806, 43)\n"
     ]
    }
   ],
   "source": [
    "# remove columns with only one unique value (all nan dummies from columns with no missing values)\n",
    "full_merged = full_merged.loc[:, full_merged.nunique(axis=0) > 1]\n",
    "dem_merged = dem_merged.loc[:, dem_merged.nunique(axis=0) > 1]\n",
    "\n",
    "print(\"Full feature dataset with constant columns dropped has shape\", full_merged.shape)\n",
    "print(\"Demographic-related feature set data with constant columns dropped has shape\", dem_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature dataset with duplicate columns dropped has shape (5806, 64)\n",
      "Demographic-related feature set data with duplicate columns dropped has shape (5806, 43)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate columns - these end up being all from nan or Not Applicable dummies \n",
    "process_features.drop_duplicate_columns(full_merged, ignore=['pop_scaled_wgt'], inplace=True)\n",
    "process_features.drop_duplicate_columns(dem_merged, ignore=['pop_scaled_wgt'], inplace=True)\n",
    "\n",
    "print(\"Full feature dataset with duplicate columns dropped has shape\", full_merged.shape)\n",
    "print(\"Demographic-related feature set data with duplicate columns dropped has shape\", dem_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicolinearity <a class=\"anchor\" id=\"colinearity\"></a>\n",
    "\n",
    "Collinearity is the presence of two or more qualities that combine linearly or have a high degree of correlation. This may affect the computation of model coefficients. Once these features are recognized, they can be gradually eliminated, leaving behind only useful, non-redundant features. One method of determining if a feature is multicollinear is by computing its (VIF). This is accomplished through the 'get_vif' function, which gives a dataframe so we may examine the features. A VIF of 1 in the outcomes denotes the absence of collinearity. A VIF greater than 1 indicates the presence of some variable collinearity. In general, the model may have issues if the VIF is substantially greater than 5 or 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# def get_vif(X, intercept_col='intercept'):\n",
    "#     if intercept_col is not None and intercept_col in X.columns:\n",
    "#         X = X.copy().drop(intercept_col, axis=1)\n",
    "    \n",
    "#     vi_factors = [variance_inflation_factor(X.values, i)\n",
    "#                              for i in range(X.shape[1])]\n",
    "    \n",
    "#     return pd.Series(vi_factors,\n",
    "#                      index=X.columns,\n",
    "#                      name='variance_inflaction_factor')\n",
    "\n",
    "# def standardize(df, numeric_only=True):\n",
    "#     if numeric_only is True:\n",
    "#     # find non-boolean columns\n",
    "#         cols = df.loc[:, df.dtypes != 'uint8'].columns\n",
    "#     else:\n",
    "#         cols = df.columns\n",
    "#     for field in cols:\n",
    "#         mean, std = df[field].mean(), df[field].std()\n",
    "#         # account for constant columns\n",
    "#         if np.all(df[field] - mean != 0):\n",
    "#             df.loc[:, field] = (df[field] - mean) / std\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# get_vif(standardize(full_merged))[40:64,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_vif(standardize(dem_merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Split and Save Data with Dummy Variables<a class=\"anchor\" id=\"save-data\"></a>\n",
    "\n",
    "As a final step, we want to split the data into training and test sets which will be used by all of the algorithms. We'll reserve 25% of the data as a test set. \n",
    "\n",
    "Each dataset to be analysed will have two associated csv files: train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into Train and Test Sets\n",
    "full_merged_train, full_merged_test = train_test_split(full_merged, \n",
    "                                       test_size=0.25,\n",
    "                                       random_state=1500,\n",
    "                                       stratify=full_merged.receive_digital_wages)\n",
    "dem_merged_train, dem_merged_test = train_test_split(dem_merged, \n",
    "                                       test_size=0.25,\n",
    "                                       random_state=1500,\n",
    "                                       stratify=dem_merged.receive_digital_wages)\n",
    "\n",
    "# Save data to files\n",
    "TRAIN_PATH, TEST_PATH = load_data.get_data_filepaths('full_merged_dumvar')\n",
    "full_merged_train.to_pickle(TRAIN_PATH)\n",
    "full_merged_test.to_pickle(TEST_PATH)\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_PATH, TEST_PATH = load_data.get_data_filepaths('dem_merged_dumvar')\n",
    "dem_merged_train.to_pickle(TRAIN_PATH)\n",
    "dem_merged_test.to_pickle(TEST_PATH)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
